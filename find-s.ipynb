{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find-S Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_csv.reader object at 0x00000276FD43B208>\n",
      "\n",
      " the given training examples are\n",
      "['sky', 'airtemp', 'humidity', 'wind', 'water', 'forcast', 'enjoysport']\n",
      "['sunny', 'warm', 'normal', 'strong', 'warm', 'same', 'yes']\n",
      "['sunny', 'warm', 'high', 'strong', 'warm', 'same', 'yes']\n",
      "['rain', 'cold', 'high', 'strong', 'warm', 'change', 'no']\n",
      "['sunny', 'warm', 'high', 'strong', 'cool', 'change', 'yes']\n",
      "\n",
      " the positive examples are : \n",
      "['sunny', 'warm', 'normal', 'strong', 'warm', 'same', 'yes']\n",
      "['sunny', 'warm', 'high', 'strong', 'warm', 'same', 'yes']\n",
      "['sunny', 'warm', 'high', 'strong', 'cool', 'change', 'yes']\n",
      "\n",
      "\n",
      "The steps of find-s are :\n",
      " ['0', '0', '0', '0', '0', '0']\n",
      "['sunny', 'warm', 'normal', 'strong', 'warm', 'same']\n",
      "['sunny', 'warm', '?', 'strong', 'warm', 'same']\n",
      "['sunny', 'warm', '?', 'strong', '?', '?']\n",
      "\n",
      " the maximal specifically hypothesis for given training example : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sunny', 'warm', '?', 'strong', '?', '?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "hypo = ['0','0','0','0','0','0']\n",
    "with open('enjoysport.csv') as csv_file:\n",
    "    readcsv = csv.reader(csv_file,delimiter=',')\n",
    "    print(readcsv)\n",
    "    data=[]\n",
    "    print(\"\\n the given training examples are\")\n",
    "    for row in readcsv:\n",
    "        print(row)\n",
    "        if row[len(row)-1].upper()=='YES':\n",
    "            data.append(row)\n",
    "\n",
    "print(\"\\n the positive examples are : \")\n",
    " \n",
    "for x in data: \n",
    "    print(x)\n",
    "print(\"\\n\")\n",
    "\n",
    "totalExamples = len(data)\n",
    "i=0\n",
    "j=0\n",
    "k=0\n",
    "\n",
    "print(\"The steps of find-s are :\\n\",hypo)\n",
    "        \n",
    "lst = []\n",
    "p=0\n",
    "d = len(data[p])-1\n",
    "\n",
    "for j in range(d):\n",
    "    lst.append(data[i][j])\n",
    "hypo = lst\n",
    "\n",
    "i = 1\n",
    "\n",
    "for i in range(totalExamples):\n",
    "    for k in range(d):\n",
    "        if hypo[k]!=data[i][k]:\n",
    "            hypo[k]=\"?\"\n",
    "            k=k+1\n",
    "        else:\n",
    "            hypo[k] = data[i][k]\n",
    "    print(hypo)\n",
    "    i=i+1\n",
    "\n",
    "print(\"\\n the maximal specifically hypothesis for given training example : \")\n",
    "lst =[]\n",
    "\n",
    "for i in range(d):\n",
    "    lst.append(hypo[i])\n",
    "lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Elimination Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sky', 'airtemp', 'humidity', 'wind', 'water', 'forcast', 'enjoysport'], ['sunny', 'warm', 'normal', 'strong', 'warm', 'same', 'yes'], ['sunny', 'warm', 'high', 'strong', 'warm', 'same', 'yes'], ['rain', 'cold', 'high', 'strong', 'warm', 'change', 'no'], ['sunny', 'warm', 'high', 'strong', 'cool', 'change', 'yes']]\n",
      "\n",
      " specific hypothesis initially  ['sunny', 'warm', 'normal', 'strong', 'warm', 'same']\n",
      "\n",
      " General hypothesis initialization  [['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']]\n",
      "\n",
      "\n",
      "if i[j]= sunny \t s[j]= sunny\n",
      "if i[j]= warm \t s[j]= warm\n",
      "if i[j]= normal \t s[j]= normal\n",
      "if i[j]= strong \t s[j]= strong\n",
      "if i[j]= warm \t s[j]= warm\n",
      "if i[j]= same \t s[j]= same\n",
      "then s[j]= same \tg[j][j]= ?\n",
      "\n",
      " after row application \n",
      " s= ['sunny', 'warm', 'normal', 'strong', 'warm', 'same']\n",
      "\n",
      "g= [['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']]\n",
      "if i[j]= sunny \t s[j]= sunny\n",
      "if i[j]= warm \t s[j]= warm\n",
      "if i[j]= high \t s[j]= normal\n",
      "if i[j]= strong \t s[j]= strong\n",
      "if i[j]= warm \t s[j]= warm\n",
      "if i[j]= same \t s[j]= same\n",
      "then s[j]= same \tg[j][j]= ?\n",
      "\n",
      " after row application \n",
      " s= ['sunny', 'warm', '?', 'strong', 'warm', 'same']\n",
      "\n",
      "g= [['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']]\n",
      "if i[j]= sunny \t s[j]= sunny\n",
      "if i[j]= warm \t s[j]= warm\n",
      "if i[j]= high \t s[j]= ?\n",
      "if i[j]= strong \t s[j]= strong\n",
      "if i[j]= cool \t s[j]= warm\n",
      "if i[j]= change \t s[j]= same\n",
      "then s[j]= ? \tg[j][j]= ?\n",
      "\n",
      " after row application \n",
      " s= ['sunny', 'warm', '?', 'strong', '?', '?']\n",
      "\n",
      "g= [['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']]\n",
      "\n",
      " Steps of candidate elimination algorithm 5\n",
      "\n",
      "s= ['sunny', 'warm', '?', 'strong', '?', '?']\n",
      "\n",
      "g= [['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']]\n",
      "\n",
      "Final specific hypothesis : ['sunny', 'warm', '?', 'strong', '?', '?']\n",
      "\n",
      " Final general hypothesis :  []\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open(\"enjoysport.csv\") as f:\n",
    "    csv_file = csv.reader(f)\n",
    "    data = list(csv_file)\n",
    "    print(data)\n",
    "s = data[1][:-1]\n",
    "print(\"\\n specific hypothesis initially \",s)\n",
    "g = [['?' for i in range(len(s))] for j in range(len(s))]\n",
    "\n",
    "print(\"\\n General hypothesis initialization \",g)\n",
    "print(\"\\n\")\n",
    "\n",
    "for i in data:\n",
    "    if i[-1]=='yes':\n",
    "        for j in range(len(s)):\n",
    "            print('if i[j]=',i[j],'\\t s[j]=',s[j])\n",
    "            if i[j]!=s[j]:\n",
    "                s[j]='?'\n",
    "                g[j][j] = '?'\n",
    "        print('then s[j]=',s[j],\"\\tg[j][j]=\",g[j][j])\n",
    "        print(\"\\n after row application \\n s=\",s)\n",
    "        print(\"\\ng=\",g)\n",
    "    elif[-1]=='no':\n",
    "        for j in range(len(s)):\n",
    "            print('in elif i[j]','\\t s[j]=',s[j])\n",
    "            \n",
    "            if i[j]!=s[j] :\n",
    "                g[j][j]=s[j]\n",
    "            else:\n",
    "                g[j][j]='?'\n",
    "print(\"\\n Steps of candidate elimination algorithm\",data.index(i)+1)\n",
    "print(\"\\ns=\",s)\n",
    "print(\"\\ng=\",g)\n",
    "gh = []\n",
    "\n",
    "for i in g:\n",
    "    for j in i:\n",
    "        if j!='?':\n",
    "            gh.append(i)\n",
    "            break\n",
    "\n",
    "print(\"\\nFinal specific hypothesis :\",s)\n",
    "print(\"\\n Final general hypothesis : \",gh)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 values of data :\n",
      "     Outlook Temperature Humidity  Windy PlayTenis\n",
      "0     Sunny         Hot     High  False        No\n",
      "1     Sunny         Hot     High   True        No\n",
      "2  Overcast         Hot     High  False       Yes\n",
      "3     Rainy        Mild     High  False       Yes\n",
      "4     Rainy        Cool   Normal  False       Yes\n",
      "\n",
      " the first 5 values of train data is\n",
      "     Outlook Temperature Humidity  Windy\n",
      "0     Sunny         Hot     High  False\n",
      "1     Sunny         Hot     High   True\n",
      "2  Overcast         Hot     High  False\n",
      "3     Rainy        Mild     High  False\n",
      "4     Rainy        Cool   Normal  False\n",
      "\n",
      " the first 5 values of train output is \n",
      " <bound method NDFrame.head of 0      No\n",
      "1      No\n",
      "2     Yes\n",
      "3     Yes\n",
      "4     Yes\n",
      "5      No\n",
      "6     Yes\n",
      "7      No\n",
      "8     Yes\n",
      "9     Yes\n",
      "10    Yes\n",
      "11    Yes\n",
      "12    Yes\n",
      "13     No\n",
      "Name: PlayTenis, dtype: object>\n",
      "\n",
      " now the train data is : \n",
      "    Outlook  Temperature  Humidity  Windy\n",
      "0        2            1         0      0\n",
      "1        2            1         0      1\n",
      "2        0            1         0      0\n",
      "3        1            2         0      0\n",
      "4        1            0         1      0\n",
      "\n",
      " now the train output is\n",
      " [0 0 1 1 1 0 1 0 1 1 1 1 1 0]\n",
      "Accuracy is : 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "data = pd.read_csv('tenisdata.csv')\n",
    "print(\"The first 5 values of data :\\n\",data.head())\n",
    "\n",
    "x = data.iloc[:,:-1]\n",
    "print(\"\\n the first 5 values of train data is\\n\",x.head())\n",
    "y=data.iloc[:,-1]\n",
    "print(\"\\n the first 5 values of train output is \\n\",y.head)\n",
    "\n",
    "le_outlook = LabelEncoder()\n",
    "x.Outlook = le_outlook.fit_transform(x.Outlook)\n",
    "le_Temperature  = LabelEncoder()\n",
    "x.Temperature = le_Temperature.fit_transform(x.Temperature)\n",
    "le_Humidity = LabelEncoder()\n",
    "x.Humidity = le_Humidity.fit_transform(x.Humidity)\n",
    "le_Windy = LabelEncoder()\n",
    "x.Windy = le_Windy.fit_transform(x.Windy)\n",
    "\n",
    "print(\"\\n now the train data is : \\n\",x.head())\n",
    "le_PlayTenis = LabelEncoder()\n",
    "y = le_PlayTenis.fit_transform(y)\n",
    "print(\"\\n now the train output is\\n\",y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.20)\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy is :\",accuracy_score(classifier.predict(X_test),y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### knn-algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target= 2 virginica PREDICTED= [2] ['virginica']\n",
      "Target= 1 versicolor PREDICTED= [1] ['versicolor']\n",
      "Target= 0 setosa PREDICTED= [0] ['setosa']\n",
      "Target= 2 virginica PREDICTED= [2] ['virginica']\n",
      "Target= 0 setosa PREDICTED= [0] ['setosa']\n",
      "Target= 2 virginica PREDICTED= [2] ['virginica']\n",
      "Target= 0 setosa PREDICTED= [0] ['setosa']\n",
      "Target= 1 versicolor PREDICTED= [1] ['versicolor']\n",
      "Target= 1 versicolor PREDICTED= [1] ['versicolor']\n",
      "Target= 1 versicolor PREDICTED= [1] ['versicolor']\n",
      "Target= 2 virginica PREDICTED= [2] ['virginica']\n",
      "Target= 1 versicolor PREDICTED= [1] ['versicolor']\n",
      "Target= 1 versicolor PREDICTED= [1] ['versicolor']\n",
      "Target= 1 versicolor PREDICTED= [1] ['versicolor']\n",
      "Target= 1 versicolor PREDICTED= [1] ['versicolor']\n",
      "Target= 0 setosa PREDICTED= [0] ['setosa']\n",
      "Target= 1 versicolor PREDICTED= [1] ['versicolor']\n",
      "Target= 1 versicolor PREDICTED= [1] ['versicolor']\n",
      "Target= 0 setosa PREDICTED= [0] ['setosa']\n",
      "Target= 0 setosa PREDICTED= [0] ['setosa']\n",
      "Target= 2 virginica PREDICTED= [2] ['virginica']\n",
      "Target= 1 versicolor PREDICTED= [1] ['versicolor']\n",
      "Target= 0 setosa PREDICTED= [0] ['setosa']\n",
      "Target= 0 setosa PREDICTED= [0] ['setosa']\n",
      "Target= 2 virginica PREDICTED= [2] ['virginica']\n",
      "Target= 0 setosa PREDICTED= [0] ['setosa']\n",
      "Target= 0 setosa PREDICTED= [0] ['setosa']\n",
      "Target= 1 versicolor PREDICTED= [1] ['versicolor']\n",
      "Target= 1 versicolor PREDICTED= [1] ['versicolor']\n",
      "Target= 0 setosa PREDICTED= [0] ['setosa']\n",
      "Target= 2 virginica PREDICTED= [2] ['virginica']\n",
      "Target= 1 versicolor PREDICTED= [1] ['versicolor']\n",
      "Target= 0 setosa PREDICTED= [0] ['setosa']\n",
      "Target= 2 virginica PREDICTED= [2] ['virginica']\n",
      "Target= 2 virginica PREDICTED= [2] ['virginica']\n",
      "Target= 1 versicolor PREDICTED= [1] ['versicolor']\n",
      "Target= 0 setosa PREDICTED= [0] ['setosa']\n",
      "Target= 1 versicolor PREDICTED= [2] ['virginica']\n",
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dataset = load_iris()\n",
    "X_train,X_test,y_train,y_test = train_test_split(dataset[\"data\"],dataset[\"target\"],random_state=0)\n",
    "\n",
    "kn = KNeighborsClassifier(n_neighbors=1)\n",
    "kn.fit(X_train,y_train)\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test[i]\n",
    "    x_new = np.array([x])\n",
    "    prediction = kn.predict(x_new)\n",
    "    print(\"Target=\",y_test[i],dataset[\"target_names\"][y_test[i]],\"PREDICTED=\",prediction,dataset[\"target_names\"][prediction])\n",
    "\n",
    "print(kn.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'GMM Classification')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import load_iris\n",
    "import sklearn.metrics as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = load_iris()\n",
    "X = pd.DataFrame(dataset.data)\n",
    "X.columns = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width']\n",
    "y=pd.DataFrame(dataset.target)\n",
    "y.columns = ['Targets']\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "colormap = np.array(['red','lime','black'])\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(X.Petal_Length,X.Petal_Width,c=colormap[y.Targets],s=40)\n",
    "plt.title('Real')\n",
    "\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit(X)\n",
    "predY = np.choose(model.labels_,[0,1,2]).astype(np.int64)\n",
    "plt.scatter(X.Petal_Length,X.Petal_Width,c=colormap[predY],s=40)\n",
    "\n",
    "scaler=preprocessing.StandardScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "xsa = scaler.transform(X)\n",
    "xs = pd.DataFrame(xsa,columns=X.columns)\n",
    "gmm= GaussianMixture(n_components=3)\n",
    "gmm.fit(xs)\n",
    "y_cluster_gmm= gmm.predict(xs)\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(X.Petal_Length,X.Petal_Width,c=colormap[y_cluster_gmm],s=40)\n",
    "plt.title('GMM Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3 Decision Tree-Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " the first 5 values of data is:\n",
      "     Outlook Temperature Humidity  Windy PlayTenis\n",
      "0     Sunny         Hot     High  False        No\n",
      "1     Sunny         Hot     High   True        No\n",
      "2  Overcast         Hot     High  False       Yes\n",
      "3     Rainy        Mild     High  False       Yes\n",
      "4     Rainy        Cool   Normal  False       Yes\n",
      "\n",
      " the first 5 values of training data is :\n",
      "     Outlook Temperature Humidity  Windy\n",
      "0     Sunny         Hot     High  False\n",
      "1     Sunny         Hot     High   True\n",
      "2  Overcast         Hot     High  False\n",
      "3     Rainy        Mild     High  False\n",
      "4     Rainy        Cool   Normal  False\n",
      "\n",
      " the first 5 values of train output data is :\n",
      " 0     No\n",
      "1     No\n",
      "2    Yes\n",
      "3    Yes\n",
      "4    Yes\n",
      "Name: PlayTenis, dtype: object\n",
      "\n",
      " now the train data is :     Outlook  Temperature  Humidity  Windy\n",
      "0        2            1         0      0\n",
      "1        2            1         0      1\n",
      "2        0            1         0      0\n",
      "3        1            2         0      0\n",
      "4        1            0         1      0\n",
      "\n",
      " now the train data (Target variable) is\n",
      " [0 0 1 1 1 0 1 0 1 1 1 1 1 0]\n",
      "\n",
      " Features in Training set is :\n",
      "     Outlook  Temperature  Humidity  Windy\n",
      "0         2            1         0      0\n",
      "1         2            1         0      1\n",
      "2         0            1         0      0\n",
      "3         1            2         0      0\n",
      "4         1            0         1      0\n",
      "5         1            0         1      1\n",
      "6         0            0         1      1\n",
      "7         2            2         0      0\n",
      "8         2            0         1      0\n",
      "9         1            2         1      0\n",
      "10        2            2         1      1\n",
      "\n",
      " test set is:\n",
      "     Outlook  Temperature  Humidity  Windy\n",
      "11        0            2         0      1\n",
      "12        0            1         1      0\n",
      "13        1            2         0      1\n",
      "\n",
      " for input\n",
      "     Outlook  Temperature  Humidity  Windy\n",
      "11        0            2         0      1\n",
      "12        0            1         1      0\n",
      "13        1            2         0      1,\n",
      " we obtain ['Yes' 'Yes' 'No']\n",
      "\n",
      " Accuracy score is : 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "data = pd.read_csv(\"tenisdata.csv\")\n",
    "print(\"\\n the first 5 values of data is:\\n\",data.head())\n",
    "X = data.iloc[:,:-1]\n",
    "print(\"\\n the first 5 values of training data is :\\n\",X.head())\n",
    "y=data.iloc[:,-1]\n",
    "print(\"\\n the first 5 values of train output data is :\\n\",y.head())\n",
    "\n",
    "le_outlook = LabelEncoder()\n",
    "X.Outlook = le_outlook.fit_transform(X.Outlook)\n",
    "le_Temperature = LabelEncoder()\n",
    "X.Temperature = le_Temperature.fit_transform(X.Temperature)\n",
    "le_Humidity = LabelEncoder()\n",
    "X.Humidity = le_Humidity.fit_transform(X.Humidity)\n",
    "le_Windy = LabelEncoder()\n",
    "X.Windy = le_Windy.fit_transform(X.Windy)\n",
    "print(\"\\n now the train data is : \",X.head())\n",
    "le_PlayTenis = LabelEncoder()\n",
    "y = le_PlayTenis.fit_transform(y)\n",
    "\n",
    "print(\"\\n now the train data (Target variable) is\\n\",y)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,shuffle=False)\n",
    "print(\"\\n Features in Training set is :\\n\",X_train)\n",
    "print(\"\\n test set is:\\n\",X_test)\n",
    "classifier = DecisionTreeClassifier(criterion='entropy')\n",
    "classifier.fit(X_train,y_train)\n",
    "pred1 = classifier.predict(X_test)\n",
    "print(\"\\n for input\\n {0},\\n we obtain {1}\".format((X_test),le_PlayTenis.inverse_transform(pred1)))\n",
    "print(\"\\n Accuracy score is :\",metrics.accuracy_score(y_test,pred1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the artificial neural network by implementing backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.46168402]\n",
      " [0.46002989]\n",
      " [0.40701276]]\n",
      "\n",
      "Loss:\n",
      "0.20110210141000082\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.55003649]\n",
      " [0.53997731]\n",
      " [0.49883742]]\n",
      "\n",
      "Loss:\n",
      "0.1307652285137512\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.61990383]\n",
      " [0.60375606]\n",
      " [0.574148  ]]\n",
      "\n",
      "Loss:\n",
      "0.08516038487969757\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.67200517]\n",
      " [0.65172708]\n",
      " [0.63158458]]\n",
      "\n",
      "Loss:\n",
      "0.05721919070062865\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.71056564]\n",
      " [0.68752403]\n",
      " [0.67466561]]\n",
      "\n",
      "Loss:\n",
      "0.03999320423057511\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.73956853]\n",
      " [0.71466316]\n",
      " [0.70733049]]\n",
      "\n",
      "Loss:\n",
      "0.029015487071670985\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.76190024]\n",
      " [0.73571931]\n",
      " [0.73260739]]\n",
      "\n",
      "Loss:\n",
      "0.02173788673782945\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.77950821]\n",
      " [0.75244119]\n",
      " [0.75260051]]\n",
      "\n",
      "Loss:\n",
      "0.016728487623777488\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.7936947 ]\n",
      " [0.7660045 ]\n",
      " [0.76874138]]\n",
      "\n",
      "Loss:\n",
      "0.01316394515291435\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.80534258]\n",
      " [0.77721034]\n",
      " [0.78201126]]\n",
      "\n",
      "Loss:\n",
      "0.010554006178085931\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.81506338]\n",
      " [0.7866163 ]\n",
      " [0.79309493]]\n",
      "\n",
      "Loss:\n",
      "0.008595818060845938\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.8232906 ]\n",
      " [0.79461956]\n",
      " [0.80248042]]\n",
      "\n",
      "Loss:\n",
      "0.007095662255043177\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.83033857]\n",
      " [0.80150945]\n",
      " [0.81052296]]\n",
      "\n",
      "Loss:\n",
      "0.005925639116282767\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.83644001]\n",
      " [0.80750118]\n",
      " [0.8174863 ]]\n",
      "\n",
      "Loss:\n",
      "0.004998877978093392\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.84177057]\n",
      " [0.81275788]\n",
      " [0.82356996]]\n",
      "\n",
      "Loss:\n",
      "0.004254870147861152\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.84646511]\n",
      " [0.81740537]\n",
      " [0.8289274 ]]\n",
      "\n",
      "Loss:\n",
      "0.0036505152298781807\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.8506288 ]\n",
      " [0.82154218]\n",
      " [0.83367843]]\n",
      "\n",
      "Loss:\n",
      "0.003154495318084738\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.85434487]\n",
      " [0.82524656]\n",
      " [0.83791795]]\n",
      "\n",
      "Loss:\n",
      "0.002743645962327075\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.85767994]\n",
      " [0.82858144]\n",
      " [0.84172205]]\n",
      "\n",
      "Loss:\n",
      "0.0024005585795127404\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.86068806]\n",
      " [0.83159804]\n",
      " [0.8451524 ]]\n",
      "\n",
      "Loss:\n",
      "0.002111961707283992\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.86341345]\n",
      " [0.83433841]\n",
      " [0.84825957]]\n",
      "\n",
      "Loss:\n",
      "0.0018676063447327753\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.86589269]\n",
      " [0.83683749]\n",
      " [0.85108535]]\n",
      "\n",
      "Loss:\n",
      "0.001659484510553191\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.86815629]\n",
      " [0.83912448]\n",
      " [0.85366463]]\n",
      "\n",
      "Loss:\n",
      "0.001481272365321441\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.87022989]\n",
      " [0.84122404]\n",
      " [0.85602675]]\n",
      "\n",
      "Loss:\n",
      "0.0013279273778803956\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.87213523]\n",
      " [0.8431571 ]\n",
      " [0.85819654]]\n",
      "\n",
      "Loss:\n",
      "0.0011953928944749266\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.87389083]\n",
      " [0.84494159]\n",
      " [0.86019521]]\n",
      "\n",
      "Loss:\n",
      "0.0010803787212970437\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.87551259]\n",
      " [0.84659291]\n",
      " [0.86204094]]\n",
      "\n",
      "Loss:\n",
      "0.000980196253507241\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.87701422]\n",
      " [0.8481244 ]\n",
      " [0.86374942]]\n",
      "\n",
      "Loss:\n",
      "0.0008926332497441071\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.87840764]\n",
      " [0.84954769]\n",
      " [0.86533427]]\n",
      "\n",
      "Loss:\n",
      "0.0008158577651894561\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.87970323]\n",
      " [0.85087291]\n",
      " [0.86680738]]\n",
      "\n",
      "Loss:\n",
      "0.0007483437675782893\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.88091008]\n",
      " [0.852109  ]\n",
      " [0.86817914]]\n",
      "\n",
      "Loss:\n",
      "0.000688813043202213\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.88203622]\n",
      " [0.85326382]\n",
      " [0.86945873]]\n",
      "\n",
      "Loss:\n",
      "0.0006361894588491062\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.88308871]\n",
      " [0.85434435]\n",
      " [0.87065424]]\n",
      "\n",
      "Loss:\n",
      "0.0005895626798396025\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.88407384]\n",
      " [0.85535678]\n",
      " [0.87177285]]\n",
      "\n",
      "Loss:\n",
      "0.0005481591857233766\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.8849972 ]\n",
      " [0.85630666]\n",
      " [0.87282095]]\n",
      "\n",
      "Loss:\n",
      "0.0005113189622899132\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.88586378]\n",
      " [0.85719893]\n",
      " [0.87380425]]\n",
      "\n",
      "Loss:\n",
      "0.000478476641484424\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.88667806]\n",
      " [0.85803804]\n",
      " [0.87472788]]\n",
      "\n",
      "Loss:\n",
      "0.00044914615096800203\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.88744407]\n",
      " [0.85882801]\n",
      " [0.87559644]]\n",
      "\n",
      "Loss:\n",
      "0.0004229081511911835\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.88816545]\n",
      " [0.85957246]\n",
      " [0.87641407]]\n",
      "\n",
      "Loss:\n",
      "0.0003993997001775197\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.88884546]\n",
      " [0.86027467]\n",
      " [0.87718452]]\n",
      "\n",
      "Loss:\n",
      "0.0003783057090868524\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.88948709]\n",
      " [0.86093763]\n",
      " [0.87791121]]\n",
      "\n",
      "Loss:\n",
      "0.000359351845325286\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89009305]\n",
      " [0.86156404]\n",
      " [0.8785972 ]]\n",
      "\n",
      "Loss:\n",
      "0.00034229861191998955\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89066579]\n",
      " [0.86215638]\n",
      " [0.87924532]]\n",
      "\n",
      "Loss:\n",
      "0.0003269363874972413\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89120756]\n",
      " [0.86271691]\n",
      " [0.87985814]]\n",
      "\n",
      "Loss:\n",
      "0.00031308125447045333\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89172041]\n",
      " [0.86324772]\n",
      " [0.880438  ]]\n",
      "\n",
      "Loss:\n",
      "0.000300571476907021\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89220625]\n",
      " [0.8637507 ]\n",
      " [0.88098706]]\n",
      "\n",
      "Loss:\n",
      "0.00028926451619578643\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89266678]\n",
      " [0.86422761]\n",
      " [0.88150729]]\n",
      "\n",
      "Loss:\n",
      "0.00027903449373000284\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89310362]\n",
      " [0.86468006]\n",
      " [0.88200051]]\n",
      "\n",
      "Loss:\n",
      "0.0002697700266009659\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89351822]\n",
      " [0.86510954]\n",
      " [0.88246839]]\n",
      "\n",
      "Loss:\n",
      "0.0002613723757133941\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89391194]\n",
      " [0.86551743]\n",
      " [0.88291249]]\n",
      "\n",
      "Loss:\n",
      "0.0002537538565106407\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89428604]\n",
      " [0.86590501]\n",
      " [0.88333422]]\n",
      "\n",
      "Loss:\n",
      "0.0002468364711945643\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89464167]\n",
      " [0.86627345]\n",
      " [0.88373493]]\n",
      "\n",
      "Loss:\n",
      "0.00024055072837383986\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89497991]\n",
      " [0.86662386]\n",
      " [0.88411582]]\n",
      "\n",
      "Loss:\n",
      "0.0002348346218112547\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89530177]\n",
      " [0.86695726]\n",
      " [0.88447805]]\n",
      "\n",
      "Loss:\n",
      "0.0002296327446287259\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89560816]\n",
      " [0.8672746 ]\n",
      " [0.88482268]]\n",
      "\n",
      "Loss:\n",
      "0.00022489551917440564\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89589995]\n",
      " [0.86757677]\n",
      " [0.88515068]]\n",
      "\n",
      "Loss:\n",
      "0.00022057852592268965\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89617796]\n",
      " [0.86786459]\n",
      " [0.88546298]]\n",
      "\n",
      "Loss:\n",
      "0.00021664191739431202\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89644294]\n",
      " [0.86813884]\n",
      " [0.88576044]]\n",
      "\n",
      "Loss:\n",
      "0.00021304990525311234\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89669558]\n",
      " [0.86840023]\n",
      " [0.88604386]]\n",
      "\n",
      "Loss:\n",
      "0.00020977031054057705\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89693655]\n",
      " [0.86864946]\n",
      " [0.886314  ]]\n",
      "\n",
      "Loss:\n",
      "0.0002067741685153578\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89716646]\n",
      " [0.86888715]\n",
      " [0.88657154]]\n",
      "\n",
      "Loss:\n",
      "0.00020403538082557461\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.8973859 ]\n",
      " [0.86911389]\n",
      " [0.88681716]]\n",
      "\n",
      "Loss:\n",
      "0.00020153040880003243\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89759539]\n",
      " [0.86933026]\n",
      " [0.88705146]]\n",
      "\n",
      "Loss:\n",
      "0.00019923800253573726\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89779546]\n",
      " [0.86953676]\n",
      " [0.88727504]]\n",
      "\n",
      "Loss:\n",
      "0.00019713896121131003\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89798658]\n",
      " [0.86973391]\n",
      " [0.88748843]]\n",
      "\n",
      "Loss:\n",
      "0.00019521592069291066\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.8981692 ]\n",
      " [0.86992216]\n",
      " [0.88769214]]\n",
      "\n",
      "Loss:\n",
      "0.00019345316503978393\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89834374]\n",
      " [0.87010195]\n",
      " [0.88788666]]\n",
      "\n",
      "Loss:\n",
      "0.00019183645897644802\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.8985106 ]\n",
      " [0.8702737 ]\n",
      " [0.88807245]]\n",
      "\n",
      "Loss:\n",
      "0.00019035289879084887\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89867016]\n",
      " [0.8704378 ]\n",
      " [0.88824993]]\n",
      "\n",
      "Loss:\n",
      "0.00018899077945311934\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89882278]\n",
      " [0.87059461]\n",
      " [0.8884195 ]]\n",
      "\n",
      "Loss:\n",
      "0.00018773947603689545\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89896879]\n",
      " [0.87074449]\n",
      " [0.88858155]]\n",
      "\n",
      "Loss:\n",
      "0.00018658933777189444\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.8991085 ]\n",
      " [0.87088776]\n",
      " [0.88873644]]\n",
      "\n",
      "Loss:\n",
      "0.00018553159326875377\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89924222]\n",
      " [0.87102474]\n",
      " [0.88888451]]\n",
      "\n",
      "Loss:\n",
      "0.00018455826564020797\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89937022]\n",
      " [0.87115572]\n",
      " [0.88902609]]\n",
      "\n",
      "Loss:\n",
      "0.0001836620964008424\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89949278]\n",
      " [0.87128097]\n",
      " [0.88916147]]\n",
      "\n",
      "Loss:\n",
      "0.00018283647716469518\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89961015]\n",
      " [0.87140077]\n",
      " [0.88929094]]\n",
      "\n",
      "Loss:\n",
      "0.000182075388278549\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89972258]\n",
      " [0.87151537]\n",
      " [0.88941479]]\n",
      "\n",
      "Loss:\n",
      "0.00018137334363221413\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89983028]\n",
      " [0.871625  ]\n",
      " [0.88953326]]\n",
      "\n",
      "Loss:\n",
      "0.00018072534097672487\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.89993348]\n",
      " [0.87172989]\n",
      " [0.88964661]]\n",
      "\n",
      "Loss:\n",
      "0.0001801268171597719\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.90003238]\n",
      " [0.87183025]\n",
      " [0.88975507]]\n",
      "\n",
      "Loss:\n",
      "0.0001795736077561576\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.90012717]\n",
      " [0.87192629]\n",
      " [0.88985886]]\n",
      "\n",
      "Loss:\n",
      "0.00017906191063085458\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.90021804]\n",
      " [0.8720182 ]\n",
      " [0.88995819]]\n",
      "\n",
      "Loss:\n",
      "0.0001785882530246666\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.90030518]\n",
      " [0.87210617]\n",
      " [0.89005327]]\n",
      "\n",
      "Loss:\n",
      "0.0001781494617986006\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.90038873]\n",
      " [0.87219037]\n",
      " [0.89014428]]\n",
      "\n",
      "Loss:\n",
      "0.00017774263651344798\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.90046887]\n",
      " [0.87227097]\n",
      " [0.8902314 ]]\n",
      "\n",
      "Loss:\n",
      "0.00017736512505667876\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.90054574]\n",
      " [0.87234813]\n",
      " [0.89031481]]\n",
      "\n",
      "Loss:\n",
      "0.0001770145015601253\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.90061949]\n",
      " [0.87242199]\n",
      " [0.89039466]]\n",
      "\n",
      "Loss:\n",
      "0.00017668854637969012\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.90069026]\n",
      " [0.87249271]\n",
      " [0.89047113]]\n",
      "\n",
      "Loss:\n",
      "0.00017638522793272447\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.90075817]\n",
      " [0.87256041]\n",
      " [0.89054435]]\n",
      "\n",
      "Loss:\n",
      "0.0001761026862103766\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.90082336]\n",
      " [0.87262523]\n",
      " [0.89061446]]\n",
      "\n",
      "Loss:\n",
      "0.00017583921780149332\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.90088593]\n",
      " [0.8726873 ]\n",
      " [0.8906816 ]]\n",
      "\n",
      "Loss:\n",
      "0.00017559326228160698\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.90094601]\n",
      " [0.87274673]\n",
      " [0.89074591]]\n",
      "\n",
      "Loss:\n",
      "0.00017536338983568036\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.90100369]\n",
      " [0.87280364]\n",
      " [0.8908075 ]]\n",
      "\n",
      "Loss:\n",
      "0.0001751482899967646\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.90105909]\n",
      " [0.87285813]\n",
      " [0.89086648]]\n",
      "\n",
      "Loss:\n",
      "0.0001749467613946714\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.9011123 ]\n",
      " [0.87291031]\n",
      " [0.89092298]]\n",
      "\n",
      "Loss:\n",
      "0.0001747577024194519\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.90116342]\n",
      " [0.87296027]\n",
      " [0.89097709]]\n",
      "\n",
      "Loss:\n",
      "0.00017458010271397602\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.90121252]\n",
      " [0.87300812]\n",
      " [0.89102892]]\n",
      "\n",
      "Loss:\n",
      "0.00017441303541843717\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.90125971]\n",
      " [0.87305393]\n",
      " [0.89107857]]\n",
      "\n",
      "Loss:\n",
      "0.0001742556500972434\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.90130506]\n",
      " [0.87309781]\n",
      " [0.89112613]]\n",
      "\n",
      "Loss:\n",
      "0.00017410716628541147\n",
      "\n",
      " input:\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "\n",
      " Actual Output:\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "Predicted output:\n",
      "[[0.90134864]\n",
      " [0.87313981]\n",
      " [0.89117168]]\n",
      "\n",
      "Loss:\n",
      "0.00017396686759791854\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array(([2,9],[1,5],[3,6]),dtype=float)\n",
    "y = np.array(([92],[86],[89]),dtype=float)\n",
    "X = X/np.amax(X,axis=0)\n",
    "y = y/100\n",
    "\n",
    "class Neural_Network(object):\n",
    "    def __init__(self):\n",
    "        self.inputSize=2\n",
    "        self.outputSize=1\n",
    "        self.hiddenSize=3\n",
    "        self.W1=np.random.randn(self.inputSize,self.hiddenSize)\n",
    "        self.W2 = np.random.randn(self.hiddenSize,self.outputSize)\n",
    "        \n",
    "        \n",
    "    def forward(self,X):\n",
    "        self.z =np.dot(X,self.W1)\n",
    "        self.z2 = self.sigmoid(self.z)\n",
    "        self.z3 = np.dot(self.z2,self.W2)\n",
    "        o=self.sigmoid(self.z3)\n",
    "        return o\n",
    "    \n",
    "    def sigmoid(self,s):\n",
    "        return 1/(1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self,s):\n",
    "        return s*(1-s)\n",
    "    \n",
    "    def backward(self,X,y,o):\n",
    "        self.o_error = y-o\n",
    "        self.o_delta= self.o_error*self.sigmoidPrime(o)\n",
    "        self.z2_error = self.o_delta.dot(self.W2.T)\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.z2)\n",
    "        self.W1+=X.T.dot(self.z2_delta)\n",
    "        self.W2+=self.z2.T.dot(self.o_delta)\n",
    "        \n",
    "    def train(self,X,y):\n",
    "        o= self.forward(X)\n",
    "        self.backward(X,y,o)\n",
    "\n",
    "NN = Neural_Network()\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    print(\"\\n input:\\n\"+str(X))\n",
    "    print(\"\\n Actual Output:\\n\"+str(y))\n",
    "    print(\"\\nPredicted output:\\n\"+str(NN.forward(X)))\n",
    "    print(\"\\nLoss:\\n\"+str(np.mean(np.square(y-NN.forward(X)))))\n",
    "    NN.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
